---
title: "PCA Visualization"
format:
  pdf:
    toc: true
    number-sections: true
    fig-cap-location: top
    fig-align: center
execute:
  warning: false
  message: false
---
```{python}

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA

# -----------------------------
# Path to your preprocessed file
# -----------------------------
DATA_PATH = r"F:\MOD002691 - FP\pi_cai_project\picai_labels\clinical_information\preprocessed_data\preprocessed_train.csv"

# -----------------------------
# Load preprocessed data
# -----------------------------
df = pd.read_csv(DATA_PATH)
print("Initial columns:", df.columns.tolist())
print("Shape:", df.shape)

# Your file should contain scaled features + label column
# In your preprocessing, you saved label as "case_csPCa" (0/1)
LABEL_COL = "case_csPCa"

if LABEL_COL not in df.columns:
    raise ValueError(f"'{LABEL_COL}' not found. Found: {df.columns.tolist()}")

print("Label distribution:", df[LABEL_COL].value_counts(dropna=False))

# Separate features and label
X = df.drop(columns=[LABEL_COL]).copy()
y = df[LABEL_COL].copy()

# Safety: keep only numeric
X = X.apply(pd.to_numeric, errors="coerce")
if X.isna().any().any():
    # If this triggers, something non-numeric got into the saved csv
    print("⚠️ NaNs found in X after numeric coercion. Dropping rows with NaNs for PCA plots.")
    keep = ~X.isna().any(axis=1)
    X = X.loc[keep]
    y = y.loc[keep]

print("Shape of feature matrix X:", X.shape)

# -----------------------------
# PCA (retain 95% variance like your example)
# If you want exactly 2 PCs: PCA(n_components=2)
# -----------------------------
pca = PCA(n_components=0.95, random_state=42)  # retain 95% variance [web:164]
X_pca = pca.fit_transform(X)

# Create PCA dataframe
pca_columns = [f"PC{i+1}" for i in range(X_pca.shape[1])]
pca_df = pd.DataFrame(X_pca, columns=pca_columns, index=X.index)
pca_df["label"] = y.values

# Optional: map names
pca_df["LabelName"] = pca_df["label"].map({0: "NO", 1: "YES"})

# -----------------------------
# Explained variance plot
# -----------------------------
explained_variance = pca.explained_variance_ratio_  # provided by sklearn PCA [web:132]
cumulative_variance = np.cumsum(explained_variance)

plt.figure(figsize=(8, 5))
plt.plot(cumulative_variance, marker="o", linestyle="-", label="Explained Variance")
plt.title("Cumulative Explained Variance (Train)")
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Variance Explained")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# -----------------------------
# PCA scatter plots (PC1 vs PC2, etc.)
# -----------------------------
num_pcs = min(4, len(pca_columns))
for i in range(num_pcs):
    for j in range(i + 1, num_pcs):
        pc_x = pca_columns[i]
        pc_y = pca_columns[j]

        plt.figure(figsize=(7, 5))
        sns.scatterplot(
            data=pca_df,
            x=pc_x, y=pc_y,
            hue="LabelName",
            palette="Set2",
            alpha=0.6
        )
        plt.title(f"PCA Scatter: {pc_x} vs {pc_y}")
        plt.xlabel(pc_x)
        plt.ylabel(pc_y)
        plt.legend(title="Label")
        plt.tight_layout()
        plt.show()

# -----------------------------
# Feature loadings (first 4 PCs)
# -----------------------------
k = min(4, len(pca_columns))
loadings = pd.DataFrame(
    pca.components_[:k].T,
    columns=pca_columns[:k],
    index=X.columns
)

plt.figure(figsize=(12, 8))
sns.heatmap(loadings, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Contributions to First PCs (Loadings)")
plt.xlabel("Principal Components")
plt.ylabel("Original Features")
plt.tight_layout()
plt.show()

# -----------------------------
# Correlation matrix of original features (scaled)
# -----------------------------
feature_corr = X.corr(method="pearson")
plt.figure(figsize=(10, 8))
sns.heatmap(feature_corr, cmap="coolwarm", annot=True, fmt=".2f", center=0)
plt.title("Correlation Matrix Between Features (Train)")
plt.tight_layout()
plt.show()


```
```{python}

from sklearn.manifold import MDS
import umap
label_map = {0: "NO", 1: "YES"}
# -----------------------------
# UMAP embedding (2D)
# -----------------------------
umap_model = umap.UMAP(
    n_neighbors=15,
    min_dist=0.1,
    n_components=2,
    random_state=42
)
X_umap = umap_model.fit_transform(X)  # standard UMAP usage [web:342][web:334]

umap_df = pd.DataFrame(X_umap, columns=["UMAP1", "UMAP2"])
umap_df["LabelName"] = y.map(label_map)

plt.figure(figsize=(7, 5))
sns.scatterplot(
    data=umap_df,
    x="UMAP1", y="UMAP2",
    hue="LabelName",
    palette="Set2",
    alpha=0.7
)
plt.title("UMAP Projection (Train clinical)")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.legend(title="Label")
plt.tight_layout()
plt.show()

# -----------------------------
# MDS embedding (2D)
# -----------------------------
mds = MDS(n_components=2, random_state=42)
X_mds = mds.fit_transform(X)  # standard sklearn API [web:338]

mds_df = pd.DataFrame(X_mds, columns=["MDS1", "MDS2"])
mds_df["LabelName"] = y.map(label_map)

plt.figure(figsize=(7, 5))
sns.scatterplot(
    data=mds_df,
    x="MDS1", y="MDS2",
    hue="LabelName",
    palette="Set2",
    alpha=0.7
)
plt.title("MDS Projection (Train clinical)")
plt.xlabel("MDS1")
plt.ylabel("MDS2")
plt.legend(title="Label")
plt.tight_layout()
plt.show()
```